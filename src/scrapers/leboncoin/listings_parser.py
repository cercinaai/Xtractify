import json
import random
import logging
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from playwright.sync_api import Page
from src.database.realStateLbc import RealStateLBCModel, save_annonce_to_db, annonce_exists
from src.utils.human_behavior import human_like_delay, human_like_scroll_to_element
from src.utils.b2_util import upload_image_to_b2

logger = logging.getLogger(__name__)
total_scraped = 0

def get_attr_by_label(ad: dict, label: str, default=None, get_values: bool = False):
    for attr in ad.get("attributes", []):
        if attr.get("key_label", "").strip() == label:
            if get_values:
                return attr.get("values_label", default) or default
            return attr.get("value_label", default)
    return default

def process_images(image_urls: list) -> list:
    new_urls = []
    for url in image_urls:
        try:
            response = requests.get(url)
            if response.status_code == 200:
                filename = url.split("/")[-1].split("?")[0]
                new_url = upload_image_to_b2(response.content, filename)
                new_urls.append(new_url)
            else:
                new_urls.append(url)
        except Exception as e:
            logger.error(f"Erreur lors du traitement de l'image {url} : {e}")
            new_urls.append(url)
    return new_urls

def extract_ads_from_html(page: Page) -> list:
    """
    Extrait les annonces de la page 1 √† partir du contenu HTML
    en analysant le script contenant l'objet __NEXT_DATA__.
    """
    content = page.content()
    soup = BeautifulSoup(content, "html.parser")
    script_tag = soup.find("script", id="__NEXT_DATA__")
    if not script_tag:
        logger.error("‚ùå Balise __NEXT_DATA__ introuvable sur la page 1.")
        return []
    try:
        data = json.loads(script_tag.string)
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du parsing du JSON: {e}")
        return []
    ads = data.get("props", {}).get("pageProps", {}).get("searchData", {}).get("ads", [])
    return ads

def intercept_ads_from_api(page: Page, timeout=60000) -> list:
    """
    Intercepte la r√©ponse POST de l'API (https://api.leboncoin.fr/finder/search)
    qui contient l'objet 'ads'. Seule la r√©ponse renvoyant des donn√©es 'ads' est retourn√©e.
    """
    try:
        with page.expect_response(
            lambda r: "api.leboncoin.fr/finder/search" in r.url and r.status == 200,
            timeout=timeout
        ) as response_info:
            # Un court d√©lai pour laisser la requ√™te s'ex√©cuter
            page.wait_for_timeout(1000)
        response = response_info.value
        try:
            data = response.json()
            if "ads" in data:
                return data.get("ads", [])
            else:
                logger.error("R√©ponse API intercept√©e sans cl√© 'ads'.")
                return []
        except Exception as je:
            logger.error(f"Erreur lors du d√©codage JSON de la r√©ponse API: {je}")
            return []
    except Exception as e:
        logger.error(f"Erreur lors de l'interception de la r√©ponse API: {e}")
        return []

def click_pagination_link(page: Page, page_number: int) -> None:
    """
    Clique sur le lien de pagination correspondant √† la page donn√©e.
    Bas√© sur l'attribut data-index pr√©sent sur le lien.
    """
    selector = f"a[data-spark-component='pagination-item'][data-index='{page_number}']"
    page_link = page.query_selector(selector)
    if not page_link:
        logger.error(f"Lien de pagination non trouv√© pour la page {page_number}.")
        return
    bbox = page_link.bounding_box()
    if bbox:
        page.mouse.move(bbox["x"] + random.uniform(2, 5), bbox["y"] + random.uniform(2, 5))
    page_link.click()

def scrape_listings_via_api(page: Page):
    """
    Scrape les annonces :
      - Page 1 : extraction via le HTML (balise __NEXT_DATA__)
      - √Ä partir de la page 2 : intercepte la r√©ponse API de la requ√™te POST (https://api.leboncoin.fr/finder/search)
    Chaque annonce est trait√©e et enregistr√©e en base.
    """
    global total_scraped
    current_page = 1
    max_pages = 5  # Limite du nombre de pages √† scraper

    while current_page <= max_pages:
        logger.info(f"‚û°Ô∏è Traitement de la page {current_page}")
        # Pour la page 1, utiliser l'extraction HTML
        if current_page == 1:
            ads = extract_ads_from_html(page)
        else:
            # Cliquer sur le lien de pagination pour d√©clencher l'appel API
            click_pagination_link(page, current_page)
            ads = intercept_ads_from_api(page)

        # Si aucune annonce n'est trouv√©e, passer √† la page suivante
        if not ads:
            logger.info(f"üèÅ Aucune annonce trouv√©e sur la page {current_page}. Passage √† la suivante...")
            current_page += 1
            human_like_delay(2, 3)
            continue

        for ad in ads:
            annonce_id = str(ad.get("list_id"))
            if annonce_exists(annonce_id):
                logger.info(f"‚è≠ Annonce {annonce_id} d√©j√† existante.")
                continue

            raw_images = ad.get("images", {}).get("urls")
            b2_images = process_images(raw_images) if raw_images else None

            annonce_data = RealStateLBCModel(
                id=annonce_id,
                publication_date=ad.get("first_publication_date"),
                title=ad.get("subject"),
                url=ad.get("url"),
                price=(ad.get("price", [None])[0] if isinstance(ad.get("price"), list) else ad.get("price")),
                nbrImages=ad.get("images", {}).get("nb_images"),
                images=b2_images,
                typeBien=get_attr_by_label(ad, "Type de bien"),
                meuble=get_attr_by_label(ad, "Ce bien est :"),
                surface=get_attr_by_label(ad, "Surface habitable"),
                nombreDepiece=get_attr_by_label(ad, "Nombre de pi√®ces"),
                nombreSalleEau=get_attr_by_label(ad, "Nombre de salle d'eau"),
                classeEnergie=get_attr_by_label(ad, "Classe √©nergie"),
                ges=get_attr_by_label(ad, "GES"),
                ascenseur=get_attr_by_label(ad, "Ascenseur"),
                etage=get_attr_by_label(ad, "√âtage de votre bien"),
                nombreEtages=get_attr_by_label(ad, "Nombre d‚Äô√©tages dans l‚Äôimmeuble"),
                exterieur=get_attr_by_label(ad, "Ext√©rieur", default=None, get_values=True),
                charges_incluses=get_attr_by_label(ad, "Charges incluses"),
                depot_garantie=get_attr_by_label(ad, "D√©p√¥t de garantie"),
                caracteristiques=get_attr_by_label(ad, "Caract√©ristiques", default=None, get_values=True),
                region=ad.get("location", {}).get("region_name"),
                city=ad.get("location", {}).get("city"),
                zipcode=ad.get("location", {}).get("zipcode"),
                agencename=ad.get("owner", {}).get("name"),
                scraped_at=datetime.utcnow()
            )
            save_annonce_to_db(annonce_data)
            total_scraped += 1
            logger.info(f"‚úÖ Annonce enregistr√©e : {annonce_id} - Total extrait : {total_scraped}")

        # Scroll et d√©lai pour simuler un comportement humain
        human_like_scroll_to_element(page, page.locator("body"), scroll_steps=random.randint(1, 3), jitter=True)
        human_like_delay(2, 3)
        logger.info(f"üåÄ Fin de la page {current_page}, passage √† la page suivante...")
        current_page += 1