import logging
from datetime import datetime
from playwright.sync_api import Page
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from src.database.realStateLbc import save_annonce_to_db, annonce_exists, RealStateLBCModel
from playwright.sync_api import expect
from src.utils.human_behavior import human_like_delay_search

logger = logging.getLogger(__name__)

def scrape_annonce_details(page: Page, ad_url: str):
    """Scrape les d√©tails d'une annonce sur Leboncoin et l'enregistre en base de donn√©es si elle n'existe pas encore."""
    try:
        # Attendre que la page soit compl√®tement charg√©e
        title_element = page.locator("h1.text-headline-1-expanded.u-break-word")
        expect(title_element).to_be_visible(timeout=60000)
        human_like_delay_search(2, 3)
        logger.info("‚úÖ Annonce ouverte avec succ√®s")

        # Si un bouton "Voir plus" est pr√©sent, cliquer dessus pour charger la description compl√®te
        voir_plus_button = page.locator("div[data-qa-id='adview_description_container'] button")
        if voir_plus_button.count() > 0:
            voir_plus_button.first.click()
            human_like_delay_search(2, 3)

        # Extraction du contenu HTML et cr√©ation d'un objet BeautifulSoup
        html_content = page.content()
        soup = BeautifulSoup(html_content, "html.parser")
        logger.info("üîç Extraction des d√©tails de l'annonce...")

        # R√©cup√©ration de l'ID de l'annonce depuis l'URL
        parsed_url = urlparse(ad_url)
        annonce_id = parsed_url.path.split("/")[-1]

        # V√©rification si l'annonce existe d√©j√† en base
        if annonce_exists(annonce_id):
            logger.info(f"üîÅ L'annonce {annonce_id} existe d√©j√† en base de donn√©es.")
            return None

        # Fonction d'extraction s√©curis√©e
        def get_text(selector, default="Pas trouv√©"):
            element = soup.select_one(selector)
            return element.get_text(strip=True) if element else default

        # Extraction des informations de base
        title = get_text("h1.text-headline-1-expanded.u-break-word", "Titre non disponible")
        price = get_text("div.flex p.text-headline-2")
        charges_comprises = get_text("div.flex p.text-caption.font-semi-bold.ml-md")
        date_publication = get_text("p.text-caption.opacity-dim-1")

         # Extraction des crit√®res
        criteria_dict = {}
        criteria_sections = soup.select("div[data-test-id='criteria']")
        for criterion in criteria_sections:
            label_elem = criterion.select_one("p.text-caption")
            value_elem = criterion.select_one("p.font-bold")
            if label_elem and value_elem:
                label = label_elem.get_text(strip=True)
                value = value_elem.get_text(strip=True)
                criteria_dict[label] = value

        # Extraction de la localisation avec plusieurs alternatives
        # Extraction de la localisation avec le nouveau s√©lecteur
        location_anchor = soup.select_one("a.text-body-1[href='#map']")
        if location_anchor:
            location_text = location_anchor.get_text(strip=True)
            parts = location_text.rsplit(" ", 1)
            if len(parts) == 2:
                city = parts[0]
                postal_code = parts[1]
            else:
                city = location_text
                postal_code = "Pas trouv√©"
        else:
            city = "Pas trouv√©"
            postal_code = "Pas trouv√©"
        logger.info(f"üìç Localisation extraite : {city} ({postal_code})")

        # Extraction des informations de l'agence avec le nouveau s√©lecteur
        agence_container = soup.select_one("div.flex.justify-between a.block.truncate")
        if agence_container:
            agence_nom = agence_container.get_text(strip=True)
        else:
            agence_nom = "Pas trouv√©"

        # Extraction de la description compl√®te
        description = get_text("div[data-qa-id='adview_description_container']", "Pas trouv√©")

        # Extraction des crit√®res compl√©mentaires avec valeurs par d√©faut
        type_vente = criteria_dict.get("Type de vente", "Pas trouv√©")
        type_bien = criteria_dict.get("Type de bien", "Pas trouv√©")
        meuble = criteria_dict.get("Ce bien est :", "Pas trouv√©")
        surface = criteria_dict.get("Surface habitable", "Pas trouv√©")
        surface_terrain = criteria_dict.get("Surface totale du terrain", "Pas trouv√©")
        nombre_pieces = criteria_dict.get("Nombre de pi√®ces", "Pas trouv√©")
        nombre_chambres = criteria_dict.get("Nombre de chambres", "Pas trouv√©")
        nombre_salles_bain = criteria_dict.get("Nombre de salle de bain", "Pas trouv√©")
        nombre_salles_eau = criteria_dict.get("Nombre de salle d'eau", "Pas trouv√©")
        nombre_niveaux = criteria_dict.get("Nombre de niveaux", "Pas trouv√©")
        # Extraction de la classe √©nerg√©tique √† partir de la liste d'ic√¥nes
        energy_criteria_container = soup.select_one("div[data-test-id='energy-criteria']")
        if energy_criteria_container:
            # Rechercher l'√©l√©ment s√©lectionn√© (par exemple, celui qui contient "drop-shadow-sm")
            selected_energy = energy_criteria_container.find("div", class_="drop-shadow-sm")
            if selected_energy:
                classe_energetique = selected_energy.get_text(strip=True)
            else:
                # En l'absence d'un √©l√©ment marqu√©, on prend le premier √©l√©ment de la liste
                first_energy = energy_criteria_container.select_one("div")
                classe_energetique = first_energy.get_text(strip=True) if first_energy else "Pas trouv√©"
        else:
            classe_energetique = "Pas trouv√©"

        # Extraction du crit√®re GES via criteria_dict
        ges = criteria_dict.get("GES", "Pas trouv√©")
        ges = criteria_dict.get("GES", "Pas trouv√©")
        annee_construction = criteria_dict.get("Ann√©e de construction", "Pas trouv√©")
        charges_honoraires = criteria_dict.get("Charges honoraires", "Pas trouv√©")
        charges_copropriete_annuelle = criteria_dict.get("Charges de copropri√©t√© annuelle", "Pas trouv√©")
        depot_garantie = criteria_dict.get("D√©p√¥t de garantie", "Pas trouv√©")
        place_parking = criteria_dict.get("Places de parking", "Pas trouv√©")
        exterieur = criteria_dict.get("Ext√©rieur", "Pas trouv√©")
        nature_bien = criteria_dict.get("Nature du bien", "Pas trouv√©")
        caracteristiques = criteria_dict.get("Caract√©ristiques", "Pas trouv√©")
        charges_incluses = criteria_dict.get("Charges incluses", "Pas trouv√©")
        reference = criteria_dict.get("R√©f√©rence", "Pas trouv√©")
        
        # Cr√©ation du mod√®le avec les donn√©es extraites
        annonce_data = RealStateLBCModel(
            id=str(annonce_id),
            title=title,
            price=price,
            charges_comprises=charges_comprises,
            date_publication=date_publication,
            location=location_text,
            code_postal=postal_code,
            city=city,
            surface=surface,
            surface_terrain=surface_terrain,
            nombre_pieces=nombre_pieces,
            nombre_chambres=nombre_chambres,
            nombre_salles_bain=nombre_salles_bain,
            nombre_salles_eau=nombre_salles_eau,
            nombre_niveaux=nombre_niveaux,
            description=description,
            type_vente=type_vente,
            type_bien=type_bien,
            meuble=meuble,
            classe_energetique=classe_energetique,
            ges=ges,
            annee_construction=annee_construction,
            charges_honoraires=charges_honoraires,
            charges_copropriete_annuelle=charges_copropriete_annuelle,
            depot_garantie=depot_garantie,
            place_parking=place_parking,
            exterieur=exterieur,
            nature_bien=nature_bien,
            caracteristiques=caracteristiques,
            charges_incluses=charges_incluses,
            reference=reference,
            agence_nom=agence_nom,
            # agence_siren=agence_siren,
            ad_url=str(ad_url),
            scraped_at=datetime.utcnow()
        )

        # Enregistrement en base de donn√©es
        save_annonce_to_db(annonce_data)
        logger.info(f"‚úÖ Annonce enregistr√©e avec succ√®s : {annonce_id}")
        
        return annonce_data

    except Exception as e:
        logger.error(f"üö® Erreur lors du scraping : {str(e)}")
        return None